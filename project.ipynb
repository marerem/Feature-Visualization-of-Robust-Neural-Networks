{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3580733",
   "metadata": {},
   "source": [
    "# Code\n",
    "Beware of computation time and file sizes... You can either run the notebook and compute most thing, or get a zipped data folder with all the heavy stuff precomputed, then you can run the notebook and it will automagically skip some computation and display the pre-computed stuff instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0970bc9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca5092-c312-498e-a52e-f86ae543d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing required python libraries\n",
    "# you can run this once and then (re-)comment it\n",
    "# !pip install -q -r reqs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a49904",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "gfvD5ool0FvK",
    "outputId": "e75026de-f0b3-4b21-bb6e-34ac30a98dd1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.models import resnet34\n",
    "import numpy as np\n",
    "from lucent.optvis import render, objectives\n",
    "from PIL import Image\n",
    "from lucent.misc.io.showing import _display_html, images\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "\n",
    "import fv_utils as fv\n",
    "from importlib import reload\n",
    "_ = reload(fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12227cb2",
   "metadata": {},
   "source": [
    "## Files prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7016c1-3885-416f-b097-2a6c9837eeff",
   "metadata": {},
   "source": [
    "Our project uses the following folder structure. If the cell below fails for some reason, make sure to manually check that you have the same folder structure and manually download any required data like the robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc68166-5f7c-46ba-b67d-2c3e0fef25c8",
   "metadata": {},
   "source": [
    "```\n",
    "PROJECT_ROOT/\n",
    "|--project.ipynb\n",
    "|--fv_utils.ipynb\n",
    "|--reqs.txt\n",
    "|--data/\n",
    "|  |--dataset/\n",
    "|  |  |...imagenet100 data from kaggle\n",
    "|  |--output/\n",
    "|  |--model/\n",
    "|  |  |--resnet34.pth.tar\n",
    "|  |--label/\n",
    "|  |  |--Labels100.json\n",
    "|  |  |--Labels1000.json\n",
    "|  |  |--100_to_1000.json\n",
    "|  |--gradient/\n",
    "|  |  |--robust_data_grad.pt\n",
    "|  |  |--standard_data_grad.pt\n",
    "|  |--adversarial_examples/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad4977-686e-47c3-8486-89fbf5a63bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = %pwd\n",
    "print(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a743f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure\n",
    "data_dir = os.path.join(ROOT, \"data\")\n",
    "if not os.path.exists(data_dir):\n",
    "    # zip archive of the data folder ~3.8 GB zipped, ~ 13 GB unzipped\n",
    "    data_dir_zip = os.path.join(ROOT, \"data.zip\")\n",
    "    if not os.path.exists(data_dir_zip):\n",
    "        print(\"Downloading\", data_dir_zip, \"...\")\n",
    "        wget.download(fv.data_folder_url, out=data_dir_zip)\n",
    "    with ZipFile(data_dir_zip, mode='r') as zip_ref:\n",
    "        zip_ref.extractall(path=ROOT)\n",
    "\n",
    "data_folders = [\n",
    "    \"model\", \"label\", \"gradient\", \"output\", \"dataset\", \"adversarial_examples\"\n",
    "]\n",
    "model_dir, label_dir, gradient_dir, output_dir, dataset_dir, advex_dir = [\n",
    "    os.path.join(data_dir, f) for f in data_folders\n",
    "]\n",
    "\n",
    "robust_resnet_34_file = os.path.join(model_dir, \"resnet34.pth.tar\")\n",
    "labels100_file = os.path.join(label_dir, \"Labels100.json\")\n",
    "labels1000_file = os.path.join(label_dir, \"Labels1000.json\")\n",
    "labels_conversion_file = os.path.join(label_dir, \"100_to_1000.json\")\n",
    "robust_gradients_file = os.path.join(gradient_dir, \"robust_data_grad.pt\")\n",
    "standard_gradients_file = os.path.join(gradient_dir, \"standard_data_grad.pt\")\n",
    "\n",
    "# check everything is here\n",
    "if (\n",
    "    os.path.exists(robust_resnet_34_file)\n",
    "    and os.path.exists(labels100_file)\n",
    "    and os.path.exists(labels1000_file)\n",
    "    and os.path.exists(labels_conversion_file)\n",
    "    and os.path.exists(robust_gradients_file)\n",
    "    and os.path.exists(standard_gradients_file)\n",
    "    and os.path.exists(output_dir)\n",
    "    and os.path.exists(dataset_dir)\n",
    "    and os.path.exists(advex_dir)\n",
    "    and len(os.listdir(dataset_dir)) == 100\n",
    "   ):\n",
    "    print(\"Ok!\")\n",
    "else:\n",
    "    print(\"You seem to be missing some files...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824246e-e31a-4468-b260-490d3c79c10c",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bd72e-f2a9-423f-a7ea-da2b6f06cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# First lets load the normal model\n",
    "standard_model = resnet34(pretrained=True).to(device).eval()\n",
    "\n",
    "# Then we load the robustly train model \n",
    "robust_model = resnet34(pretrained=True)\n",
    "states_dict = torch.load(robust_resnet_34_file, map_location=device)\n",
    "new_dict = {}\n",
    "for k in states_dict['model']:\n",
    "    new_dict[\".\".join(k.split(\".\")[1:])] = states_dict['model'][k]\n",
    "    \n",
    "#loading the model and copying it in the ancient model \n",
    "robust_model.load_state_dict(new_dict)\n",
    "_= robust_model.to(device).eval()\n",
    "\n",
    "del new_dict, states_dict\n",
    "fv.cleanMemGPU()\n",
    "\n",
    "robust_model.name = \"Robust model\"\n",
    "standard_model.name = \"Non-robust model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca074b",
   "metadata": {},
   "source": [
    "## Print some models info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LkIyun5hJ55D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "LkIyun5hJ55D",
    "outputId": "78d85ed4-d83d-4c39-f6f0-43b704d28bf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print summary of layers, same for robust and non-robust\n",
    "fv.print_model_info(standard_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177af8a",
   "metadata": {},
   "source": [
    "## Visualize some channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf8881-7262-4eee-af06-98bf4f533b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"layer1:0\", \"layer2:2\", \"layer3:2\", \"layer3:4\", \"layer4:2\"]\n",
    "# visualize some channels thanks to lucent FV rendering function\n",
    "fv.visualize_channels(channels, standard_model, output_dir)\n",
    "fv.visualize_channels(channels, robust_model, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1646b",
   "metadata": {},
   "source": [
    "## DeepDream objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81df4b",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e95dd-3a10-4039-830c-dc40e6d39269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucent.optvis.objectives import wrap_objective\n",
    "# Deep dream objective function \n",
    "@wrap_objective()\n",
    "def deepdream(layer_name):\n",
    "    def inner(model):\n",
    "        return -(model(layer_name) ** 2).mean() # negative optimized (to maximize loss)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bb6b3",
   "metadata": {},
   "source": [
    "### Parametrization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucent.optvis import param\n",
    "# CPPN parametrization\n",
    "cppn_param_f = lambda: param.cppn(128)\n",
    "# optimizer with lower learning rate for CPPN\n",
    "cppn_opt = lambda params: torch.optim.Adam(params, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015eb2-33b8-4208-93a0-b8f3bbee8362",
   "metadata": {},
   "source": [
    "### Visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4947f2e-4f91-4794-a84e-b6615114d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_deepdream(model, layer, param_f=None, optimizer=None, f_ext=\"\"):\n",
    "    # Visualize layer with deep dream objective for non-robust model\n",
    "    f = os.path.join(output_dir, model.name+\"_\"+layer+\"_\"+f_ext+\"_deepdream_sequence.pt\")\n",
    "    if os.path.exists(f):\n",
    "        sequence = torch.load(f)\n",
    "    else:\n",
    "        sequence = np.array(render.render_vis(\n",
    "            model=model,\n",
    "            objective_f=deepdream(layer),\n",
    "            param_f=param_f,\n",
    "            optimizer=optimizer,\n",
    "            transforms=[],\n",
    "            show_inline=False,\n",
    "            show_image=False,\n",
    "            thresholds=range(512)\n",
    "        ))\n",
    "        torch.save(sequence, f)\n",
    "        \n",
    "    shape = sequence.shape\n",
    "    return sequence.reshape(shape[0], shape[2], shape[3], shape[4])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ece215",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c177a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "w1mTLljf5utK",
    "outputId": "0dcbfcdf-52ab-4dc0-a402-93e55fbe8380"
   },
   "outputs": [],
   "source": [
    "standard_sequence_cppn = visualize_deepdream(\n",
    "    standard_model, \"layer4_2\", param_f=cppn_param_f, optimizer=cppn_opt, f_ext=\"cppn\"\n",
    ")\n",
    "robust_sequence_cppn = visualize_deepdream(\n",
    "    robust_model, \"layer4_2\", param_f=cppn_param_f, optimizer=cppn_opt, f_ext=\"cppn\"\n",
    ")\n",
    "standard_sequence = visualize_deepdream(standard_model, \"layer4_2\")\n",
    "robust_sequence = visualize_deepdream(robust_model, \"layer4_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4da399",
   "metadata": {
    "id": "1d4da399"
   },
   "outputs": [],
   "source": [
    "_display_html(\n",
    "    fv.html_sequence(standard_sequence_cppn) + \n",
    "    fv.html_sequence(standard_sequence) + \n",
    "    fv.html_sequence(robust_sequence_cppn) + \n",
    "    fv.html_sequence(robust_sequence)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70503c7",
   "metadata": {},
   "source": [
    "## Adversarial attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf5319",
   "metadata": {},
   "source": [
    "### Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ee0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Convert from ImageNet100 code to actual label\n",
    "with open(labels100_file) as json_file:\n",
    "    code_to_label_100 = json.load(json_file)\n",
    "# Convert from model output (ImageNet1000) code to actual label\n",
    "with open(labels1000_file) as json_file:\n",
    "    code_to_label_1000 = json.load(json_file)\n",
    "# convert 100 index to 1000 index\n",
    "with open(labels_conversion_file) as json_file:\n",
    "    labels_100_to_1000 = json.load(json_file)\n",
    "\n",
    "def indexFromCode(code):\n",
    "    return torch.tensor(labels_100_to_1000[code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49b806-fcaa-4192-87eb-53c372680360",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b7c9c-d7f7-477b-a0e4-6a276f8b1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = list(DataLoader(\n",
    "    # limit to 100 examples\n",
    "    fv.ImageNet100ValDataset(dataset_dir, 100),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fbe00-9bf4-45c5-aa00-281909448bea",
   "metadata": {},
   "source": [
    "### Display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95d617-2bd7-46f3-a88d-37453844a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "for i in range(100):  \n",
    "    # get image and label from dataloader\n",
    "    image, features, labels = dataloader[i]\n",
    "    image = image[0].squeeze()\n",
    "    features = features.to(device)\n",
    "    label = code_to_label_100[labels[0]]\n",
    "    \n",
    "    # get model output\n",
    "    with torch.no_grad():\n",
    "        std_output = standard_model(features)\n",
    "        rob_output = robust_model(features)\n",
    "        std_output_label = code_to_label_1000[str(std_output.argmax().item())]\n",
    "        rob_output_label = code_to_label_1000[str(rob_output.argmax().item())]\n",
    "\n",
    "    # stop when prediction difference\n",
    "    if (rob_output_label == label and rob_output_label != std_output_label) or True:\n",
    "        # display output\n",
    "        image = T.ToPILImage()(image)\n",
    "        print(f\"Correct label   : {label}\")\n",
    "        print(f\"Non robust model label : {std_output_label}\")\n",
    "        print(f\"Robust model label : {rob_output_label}\")\n",
    "        display(image)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8fc9e",
   "metadata": {},
   "source": [
    "### Checking predcition accuracies\n",
    "Takes some time, you can skip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f93205-5872-41eb-ad74-4f2a61387b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    fv.ImageNet100ValDataset(dataset_dir),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "data_len = len(dataloader)\n",
    "\n",
    "def acc(model):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for _, data, target in tqdm(dataloader):\n",
    "            output = model(data.to(device))\n",
    "            idx = output.argmax().item()\n",
    "            correct += code_to_label_1000[str(idx)] == code_to_label_100[target[0]]\n",
    "        return 100 * correct / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the test set for robust model: 79.60%\n",
    "# Accuracy on the test set for standard model: 79.72%\n",
    "# set this to True to compute accuracies, but it's time-consuming and not very useful\n",
    "compute=False\n",
    "if compute:\n",
    "    ra = acc(robust_model)\n",
    "    print('Accuracy on the test set for robust model: {:.2f}%'.format(ra))\n",
    "    sa = acc(standard_model)\n",
    "    print('Accuracy on the test set for standard model: {:.2f}%'.format(sa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef9de6",
   "metadata": {},
   "source": [
    "### FGSM attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30837bd",
   "metadata": {},
   "source": [
    "$$x^{adv} = x + \\epsilon \\cdot sign(\\nabla_x J(x, y_{true})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd2559-dc31-47b0-9a7c-bf1e95ab0ba4",
   "metadata": {},
   "source": [
    "#### Attack function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # data_grad already contains the sign() of the gradients (pre-computed)\n",
    "    return image + epsilon * data_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da1d0a-e1e6-4bfb-90ad-7d6d3bb9d6da",
   "metadata": {},
   "source": [
    "#### Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9bb04-8727-4bad-9490-632c98a8c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_gradients(model):\n",
    "    print(model.name)\n",
    "    \n",
    "    data_grads = []\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for _, data, target in tqdm(dataloader):\n",
    "        # Send the data and label to the device\n",
    "        data = data.to(device)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        # Calculate the loss\n",
    "        F.nll_loss(\n",
    "            output,\n",
    "            torch.tensor((indexFromCode(target[0]),)).to(device)\n",
    "        # Calculate gradients of model in backward pass\n",
    "        ).backward()\n",
    "        # Collect gradient signs\n",
    "        data_grads.append(data.grad.data.sign())\n",
    "        \n",
    "        # enable this at each iteration if you have memory issues (much slower)\n",
    "        # del data, output\n",
    "        # fv.cleanMemGPU()\n",
    "        \n",
    "    # Return the collected data_grads\n",
    "    print()\n",
    "    return data_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once, requires ~ min. 4GB of memory\n",
    "# set to True to compute, or dl the precomputed gradients\n",
    "compute=False\n",
    "if compute:\n",
    "    # robust\n",
    "    fv.cleanMemGPU()\n",
    "    robust_grad_data = compute_gradients(robust_model)\n",
    "    torch.save(robust_grad_data, robust_gradients_file)\n",
    "    del robust_grad_data\n",
    "    fv.cleanMemGPU()\n",
    "    # standard\n",
    "    standard_grad_data = compute_gradients(standard_model)\n",
    "    torch.save(standard_grad_data, standard_gradients_file)\n",
    "    del standard_grad_data\n",
    "    fv.cleanMemGPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9006b4",
   "metadata": {},
   "source": [
    "#### Test epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ba02d-f991-412e-9ee0-aab00ec3ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    fv.ImageNet100ValDataset(dataset_dir),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "data_len = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epsilons(model, epsilons, data_grads):\n",
    "    with torch.no_grad():\n",
    "        print(model.name, \"with epsilon limit of\", epsilons[-1])\n",
    "        \n",
    "        # Accuracy counter\n",
    "        accuracies = []\n",
    "        eps_len = len(epsilons)\n",
    "        for i, e in enumerate(epsilons):\n",
    "            correct = 0\n",
    "            # Loop over all examples in test set\n",
    "            for j, (_, data, target) in enumerate(dataloader):\n",
    "                perturbed_image = fgsm_attack(\n",
    "                    data.to(device),\n",
    "                    e,\n",
    "                    data_grads[j].to(device)\n",
    "                )\n",
    "                output = model(perturbed_image)\n",
    "                # Check for success\n",
    "                correct += (\n",
    "                    code_to_label_1000[str(output.argmax().item())]\n",
    "                    == \n",
    "                    code_to_label_100[target[0]]\n",
    "                )\n",
    "                print(\n",
    "                    \"Progress:\", j + 1, \"/\", data_len,\n",
    "                    \"\\tEpsilons\", i + 1, \"/\", eps_len, end=\"\\r\"\n",
    "                )\n",
    "                # uncomment this if memory issue, much slower\n",
    "                # del perturbed_image, output\n",
    "                # fv.cleanMemGPU()\n",
    "                \n",
    "            # Calculate final accuracies for each epsilons\n",
    "            accuracies.append(correct / data_len)\n",
    "    \n",
    "        # Return the accuracies\n",
    "        print()\n",
    "        return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1f41d-04ac-4d85-a9e1-513678090d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot the epsilons\n",
    "def plot_epsilons(epsilon_limit, num_samples=50):\n",
    "    file_name = \"epsilon_accuracies_{}\".format(epsilon_limit)\n",
    "    accuracy_file = os.path.join(output_dir, file_name)\n",
    "    # log range between 0 and epsilon_limit\n",
    "    epsilon_range = np.logspace(0, 1, num=num_samples) - 1\n",
    "    epsilon_range = epsilon_range / ((1/epsilon_limit) * epsilon_range[-1])\n",
    "    if not os.path.exists(accuracy_file):\n",
    "        fv.cleanMemGPU()\n",
    "        rob_gradients = torch.load(robust_gradients_file, map_location=\"cpu\")\n",
    "        epsilon_accuracies_robust = test_epsilons(\n",
    "            robust_model, epsilon_range, rob_gradients\n",
    "        )\n",
    "        del rob_gradients\n",
    "        fv.cleanMemGPU()\n",
    "        std_gradients = torch.load(standard_gradients_file, map_location=\"cpu\")\n",
    "        epsilon_accuracies_standard = test_epsilons(\n",
    "            standard_model, epsilon_range, std_gradients\n",
    "        )\n",
    "        del std_gradients\n",
    "        fv.cleanMemGPU()\n",
    "        accuracies = [\n",
    "            epsilon_accuracies_robust,\n",
    "            epsilon_accuracies_standard\n",
    "        ]\n",
    "        torch.save(accuracies, accuracy_file)\n",
    "    accuracies = torch.load(accuracy_file)\n",
    "    for i, accs in enumerate(accuracies):\n",
    "        plt.plot(\n",
    "            epsilon_range, accs, \n",
    "            label=(robust_model.name if i == 0 else standard_model.name)\n",
    "        )\n",
    "    plt.title(\"Model accuracies w.r.t to FGSM attack epsilons\")\n",
    "    plt.xlabel(\"epsilon\")\n",
    "    plt.xscale(\"logit\")\n",
    "    plt.xticks(ticks=epsilon_range[::max(len(epsilon_range)//10, 1)], minor=False)\n",
    "    plt.tick_params(\n",
    "        axis=\"x\",\n",
    "        labelrotation=90,\n",
    "        grid_color=\"black\",\n",
    "        grid_alpha=0.2,\n",
    "        grid_linewidth=1\n",
    "    )\n",
    "    plt.grid(visible=True)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13a599-e189-49e9-bde7-02d24f7f6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an upper bound for epsilon\n",
    "epsilon_limits = [3e-1]\n",
    "# default: logspace of 50 epsilons between 0 and epsilon_limit included\n",
    "# change this by setting the num_samples to whatever value you prefer below\n",
    "# takes a GPU and a lot time with 50...\n",
    "compute=False\n",
    "if compute:\n",
    "    for el in epsilon_limits:\n",
    "        plot_epsilons(el, num_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e57d7c-5c7c-4fcc-a25f-d8538f84655a",
   "metadata": {},
   "source": [
    "#### Display attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002da0a-7b46-40b7-aa61-fb19b45261c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    fv.ImageNet100ValDataset(dataset_dir),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "data_len = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc567890-2ae6-4a35-800c-52c5acc4686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(model, epsilon, grad_sign, advex_number=10, image_id=None):\n",
    "    with torch.no_grad():\n",
    "        print(model.name, \"with epsilon of\", epsilon)\n",
    "\n",
    "        advex = dict()\n",
    "        # Loop over all examples in test set\n",
    "        for i, (image, data, target) in enumerate(tqdm(dataloader)):\n",
    "            if image_id == None or i == image_id:\n",
    "                data = data.to(device)\n",
    "                true_label = code_to_label_100[target[0]]\n",
    "                original_idx_1000 = str(model(data).argmax().item())\n",
    "                original_prediction = code_to_label_1000[original_idx_1000]\n",
    "    \n",
    "                # predict pertured image\n",
    "                perturbed_data = fgsm_attack(\n",
    "                    data,\n",
    "                    epsilon,\n",
    "                    grad_sign[i].to(device)\n",
    "                )\n",
    "                perturbed_idx_1000 = str(model(perturbed_data).argmax().item())\n",
    "                perturbed_prediction = code_to_label_1000[perturbed_idx_1000]\n",
    "                \n",
    "                # Check for success\n",
    "                # accuracy += true_label == perturbed_prediction\n",
    "                if (\n",
    "                    (image_id != None or perturbed_prediction != true_label)\n",
    "                    and original_prediction == true_label\n",
    "                    and len(advex) < advex_number\n",
    "                ):\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    advex[i] = (true_label, perturbed_prediction, adv_ex)\n",
    "                if len(advex) >= advex_number:\n",
    "                    break\n",
    "\n",
    "        # Return the accuracies\n",
    "        print()\n",
    "        return advex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dc8c0-293c-4527-b217-da138e625671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values from the paper on fgsm\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "def denorm(img):\n",
    "    return ((std * img.T) + mean).T\n",
    "\n",
    "def plot_stack(stack, epsilon, cmap=\"hsv\"):\n",
    "    stack_len = len(stack)\n",
    "    cols = 3\n",
    "    rows = (stack_len // cols) + 1\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(20, 20))\n",
    "    for i in range(stack_len):\n",
    "        fig.add_subplot(rows, cols, i+1)\n",
    "        plt.imshow(np.transpose(denorm(stack[i][2]), (1, 2, 0)), cmap=cmap)\n",
    "        plt.title(\"Epsilon: {}\\nOriginal Label: {}\\nPredicted Label: {}\".format(\n",
    "            epsilon, stack[i][0], stack[i][1]\n",
    "        ))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1387ace-e9c4-44eb-9f41-aa2f8c97691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _advex(epsilon=0.1, advex_number=100, image_id=None):\n",
    "    fv.cleanMemGPU()\n",
    "    rob_grad_sign = torch.load(robust_gradients_file, map_location=\"cpu\")\n",
    "    rob_advex = attack(robust_model, epsilon, rob_grad_sign, advex_number=advex_number, image_id=image_id)\n",
    "    del rob_grad_sign\n",
    "    fv.cleanMemGPU()\n",
    "    std_grad_sign = torch.load(standard_gradients_file, map_location=\"cpu\")\n",
    "    std_advex = attack(standard_model, epsilon, std_grad_sign, advex_number=advex_number, image_id=image_id)\n",
    "    del std_grad_sign\n",
    "    fv.cleanMemGPU()\n",
    "    print(\"Collected\", advex_number, \"adversarial examples.\")\n",
    "    return rob_advex, std_advex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabccd14-800e-45c8-81b7-5de1ef3c2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "# Display the perturbed image, original label, and predicted label\n",
    "\n",
    "# select a specific image_id to analyze, and set the number to 1\n",
    "# eg, 115 for the flamingos that are not flamingos\n",
    "epsilon = 0.1\n",
    "advex_number = 100\n",
    "image_id = None\n",
    "ra, sa = _advex(epsilon=epsilon, advex_number=advex_number, image_id=image_id)\n",
    "\n",
    "number_of_examples_to_display = 3 # per model\n",
    "ex = []\n",
    "ks = list(ra.keys())\n",
    "shuffle(ks)\n",
    "for k in ks:\n",
    "    if k in sa:\n",
    "        ex.append(ra[k])\n",
    "        ex.append(sa[k])\n",
    "    if len(ex) >= 2*number_of_examples_to_display:\n",
    "        break\n",
    "plot_stack(ex, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506008e-9d0e-4e29-9379-c03a2ca8161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this code if you don't have the last data.zip version with the flamingos\n",
    "\n",
    "# image 115\n",
    "# flamingos = []\n",
    "# ex = []\n",
    "# es = [0, 0.01, 0.35]\n",
    "# image_id = 115\n",
    "# advex_number = 1\n",
    "# for epsilon in es:\n",
    "#     flamingos.append(_advex(epsilon=epsilon, advex_number=advex_number, image_id=image_id))\n",
    "\n",
    "# for f in flamingos:\n",
    "#     ex.append(f[0][image_id])\n",
    "# for f in flamingos:\n",
    "#     ex.append(f[1][image_id])\n",
    "\n",
    "# plot_stack(ex, epsilon)\n",
    "\n",
    "flamingo_std = [\n",
    "    os.path.join(output_dir, \"flamingo-e0.png\"),\n",
    "    os.path.join(output_dir, \"flamingo-e0.01.png\"),\n",
    "    os.path.join(output_dir, \"flamingo-e0.35.png\")\n",
    "]\n",
    "flamingo_rob = [\n",
    "    os.path.join(output_dir, \"flamingo_rob_e0.png\"),\n",
    "    os.path.join(output_dir, \"flamingo_rob_e0.01.png\"),\n",
    "    os.path.join(output_dir, \"flamingo_rob_e0.35.png\")\n",
    "]\n",
    "\n",
    "s = []\n",
    "for f in flamingo_rob:\n",
    "    s.append(Image.open(f).convert(\"RGB\"))\n",
    "images(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729070b-7be4-4d97-bd89-0fda2c989219",
   "metadata": {},
   "source": [
    "#### Visualization gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f26af-781b-4267-8419-c9b610ece0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "flamingos = []\n",
    "ex = []\n",
    "es = [0, 0.01, 0.35]\n",
    "image_id = 115\n",
    "advex_number = 1\n",
    "for epsilon in es:\n",
    "    flamingos.append(_advex(epsilon=epsilon, advex_number=advex_number, image_id=image_id))\n",
    "\n",
    "for f in flamingos:\n",
    "    ex.append(f[0][image_id])\n",
    "for f in flamingos:\n",
    "    ex.append(f[1][image_id])\n",
    "\n",
    "plot_stack(ex, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b34606-d668-44ad-a596-9d262d5ec1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "per = flamingos[1][0][115][2] - flamingos[0][0][115][2]\n",
    "per[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4f78b-a5fd-4c70-8d19-642a12f96394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(per/0.3, (1, 2, 0))) # grad of image\n",
    "plt.show()\n",
    "plt.imshow(per[0]/0.3) # channel 1\n",
    "plt.show()\n",
    "plt.imshow(per[1]/0.3) # channel 2\n",
    "plt.show()\n",
    "plt.imshow(per[2]/0.3) # # channel 3\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25061107-4e20-44f7-ace7-1730d268217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = per[1].flatten()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(pixels, bins=256, range=(-0.4, 0.4), color='gray', alpha=0.9)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b62d1-9a73-46f2-918c-26cbdfdbdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = (per[0].flatten()/0.3)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(pixels, bins=100, color='gray', alpha=0.5)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b0998",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a035a5",
   "metadata": {},
   "source": [
    "### Load adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba633a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get adversarial examples\n",
    "# advex_dir = os.path.join(data_dir, \"adversarial_examples\")\n",
    "\n",
    "# advex_std_01 = torch.load(os.path.join(advex_dir, \"adv_example_normal_1.pt\"))\n",
    "# advex_std_01_dir = os.path.join(advex_dir, \"adv_example_normal_1\")\n",
    "# os.makedirs(advex_std_01_dir, exist_ok=True)\n",
    "\n",
    "# # Make separate into new files to avoid memory problems\n",
    "# for i in range(len(advex_std_01)):\n",
    "#     torch.save(advex_std_01[i], os.path.join(advex_std_01_dir, f'{i}.pt'))\n",
    "\n",
    "# del advex_std_01\n",
    "# fv.cleanMemGPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3cfef",
   "metadata": {},
   "source": [
    "### Compute neuron sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import \\\n",
    "    create_feature_extractor, get_graph_node_names\n",
    "\n",
    "# Initialize data loader\n",
    "image_loader = DataLoader(\n",
    "    fv.ImageNet100ValDataset(dataset_dir),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "data_len = len(image_loader)\n",
    "        \n",
    "def compute_neuron_sensitivity(model, adversarial_examples):\n",
    "    with torch.no_grad():\n",
    "        adv_len = len(adversarial_examples)\n",
    "        \n",
    "        # Make model output all neuron outputs\n",
    "        _, eval_nodes = get_graph_node_names(model)  # get model's layer names\n",
    "        neurons_model = create_feature_extractor(model, return_nodes=eval_nodes) \n",
    "\n",
    "        # initialize neuron sensitivities to be empty\n",
    "        sensitivities_sum = {param: 0 for param in eval_nodes}\n",
    "        \n",
    "        # Loop over all examples in image set and adversarial examples\n",
    "        for i, (image, data, target) in enumerate(image_loader):\n",
    "            if i >= adv_len:\n",
    "                break\n",
    "            # Outputs\n",
    "            img_neurons = neurons_model(data.to(device))\n",
    "            # adversarial example corresponding to data\n",
    "            advex_neurons = neurons_model(adversarial_examples[i].to(device))\n",
    "            \n",
    "            # Sum sensitivities from one image to the next layer by layer\n",
    "            sensitivities_sum = {\n",
    "                key: sensitivities_sum[key] + torch.abs(img_neurons[key] - advex_neurons[key]) \n",
    "                for key in sensitivities_sum\n",
    "            }\n",
    "            \n",
    "            # clear cache\n",
    "            # del img_neurons\n",
    "            # del advex_neurons\n",
    "            # fv.cleanMemGPU()\n",
    "        \n",
    "        # Average sensitivities over all images and normalize by number of elements in layer\n",
    "        neuron_sensitivities = {\n",
    "            key: sensitivities_sum[key] / (data_len * torch.numel(sensitivities_sum[key])) \n",
    "            for key in sensitivities_sum\n",
    "        }\n",
    "            \n",
    "        return neuron_sensitivities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9365a",
   "metadata": {},
   "source": [
    "#### Non-robust model, epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce97a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advex_dir = os.path.join(data_dir, \"adversarial_examples\")\n",
    "# advex_std_01_dir = os.path.join(advex_dir, \"adv_example_normal_1\")\n",
    "# std_sensitivities_file = os.path.join(output_dir, 'sensitivities_std_01.pkl')\n",
    "# fv.cleanMemGPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623c7c7-5d89-4814-b0a3-990bf5d5e4f1",
   "metadata": {},
   "source": [
    "#### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cde6c-bd41-430f-81d7-17dec48727d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# advex_std = []\n",
    "# for i in range(data_len):\n",
    "#     advex_std.append(torch.load(os.path.join(advex_std_01_dir, f'{i}.pt')))\n",
    "\n",
    "# display(advex_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa47ba4-36a6-4505-b5ae-2828fe1b801e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flamingos = []\n",
    "# ex = []\n",
    "# epsilon=0\n",
    "# advex_number = 1\n",
    "# r = range(100, 150)\n",
    "\n",
    "# for i in r:\n",
    "#     print(i)\n",
    "#     flamingos.append(_advex(epsilon=epsilon, advex_number=advex_number, image_id=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739828e-1270-4006-8ae5-548351d25808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flamingos_advex(epsilons):\n",
    "    for e in epsilons:\n",
    "        print(\"Advexes for epsilon =\", e)\n",
    "        flamingo_file = os.path.join(advex_dir, \"flamingos_advex_e\"+str(e)+\".pt\")\n",
    "        if not os.path.exists(flamingo_file):\n",
    "            flamingos = []\n",
    "            # flamingo range\n",
    "            r = range(100, 150)\n",
    "            for i in r:\n",
    "                flamingos.append(\n",
    "                    _advex(epsilon=e, advex_number=1, image_id=i)\n",
    "                )\n",
    "            torch.save(flamingos, flamingo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efafce-89d4-4a28-8adf-5a42c7d9e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.01, 0.1]\n",
    "flamingos_advex(epsilons)\n",
    "std_advex = dict()\n",
    "rob_advex = dict()\n",
    "for e in epsilons:\n",
    "    flamingo_file = os.path.join(advex_dir, \"flamingos_advex_e\"+str(e)+\".pt\")\n",
    "    flamingos = torch.load(flamingo_file)\n",
    "    std_advex[e] = []\n",
    "    for f in flamingos:\n",
    "        l = list(f[1].values())\n",
    "        if len(l) > 0:\n",
    "            tu = l[0]\n",
    "            t = torch.as_tensor([tu[2]])\n",
    "            std_advex[e].append(t)\n",
    "        else:\n",
    "            print(len(std_advex[e]), l)\n",
    "    rob_advex[e] = []\n",
    "    for f in flamingos:\n",
    "        l = list(f[0].values())\n",
    "        if len(l) > 0:\n",
    "            tu = l[0]\n",
    "            t = torch.as_tensor([tu[2]])\n",
    "            rob_advex[e].append(t)\n",
    "        else:\n",
    "            print(len(rob_advex[e]), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e890ae7-bdc6-4c88-accd-cd410a75bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(std_advex))\n",
    "# std_sensitivities = compute_neuron_sensitivity(standard_model, std_advex)\n",
    "#display(sensitivities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "std_sensitivities_file = os.path.join(output_dir, 'sensitivities_std_01.pkl')\n",
    "fv.cleanMemGPU()\n",
    "if not os.path.exists(std_sensitivities_file):\n",
    "    sensitivities = compute_neuron_sensitivity(standard_model, advex_std)\n",
    "    with open(sensitivities_file, 'wb') as f:\n",
    "        pickle.dump(sensitivities, f)\n",
    "    del sensitivities\n",
    "    fv.cleanMemGPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d56bc",
   "metadata": {},
   "source": [
    "#### Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c61bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(std_sensitivities_file, 'rb') as f:\n",
    "    loaded_sensitivities = pickle.load(f)\n",
    "\n",
    "print(len(loaded_sensitivities))\n",
    "# print(loaded_sensitivities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11617c7d",
   "metadata": {},
   "source": [
    "### Find k most sensitive neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb543a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sensitivities_per_layer(sensitivities, k):\n",
    "    topk = torch.zeros(k)  # most sensitive neurons value\n",
    "    topk_idx = [0] * k  # name of layer + index of most sensitive neurons\n",
    "    \n",
    "    for layer_name, tensor in sensitivities.items():\n",
    "        if layer_name == \"flatten\":\n",
    "            continue\n",
    "        # find top k in layer\n",
    "        topk_tensor, _ = torch.topk(torch.flatten(tensor), k)\n",
    "        \n",
    "        for val in topk_tensor:\n",
    "            # check if bigger than something in topk\n",
    "            bigger = False\n",
    "            for top in topk:\n",
    "                if val > top:\n",
    "                    bigger = True\n",
    "            \n",
    "            if bigger:\n",
    "                min_idx = torch.argmin(topk)  # get index\n",
    "                \n",
    "                # replace values\n",
    "                topk[min_idx] = val\n",
    "                val_idx = (tensor == val).nonzero(as_tuple=False)\n",
    "                topk_idx[min_idx] = (layer_name, val_idx.flatten())\n",
    "    \n",
    "    return topk, topk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cbd19-2e4c-4be9-90db-267ffb5991ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = reload(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "for e in epsilons:\n",
    "    rob_sens = compute_neuron_sensitivity(robust_model, rob_advex[e])\n",
    "    std_sens = compute_neuron_sensitivity(standard_model, std_advex[e])\n",
    "    k_val_rob, k_id_rob = top_k_sensitivities_per_layer(rob_sens, k)\n",
    "    k_val_std, k_id_std = top_k_sensitivities_per_layer(std_sens, k)\n",
    "    channels = []\n",
    "    for s in k_id_rob:\n",
    "        channels.append(s[0] + \":\" + str(s[1][1].item()))\n",
    "    print(\"Robust visualization with epsilon:\", e)\n",
    "    fv.visualize_channels(channels, robust_model, output_dir, epsilon=e)\n",
    "    channels = []\n",
    "    for s in k_id_std:\n",
    "        channels.append(s[0] + \":\" + str(s[1][1].item()))\n",
    "    print(\"Non Robust visualization with epsilon:\", e)\n",
    "    fv.visualize_channels(channels, standard_model, output_dir, epsilon=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800fba9-167e-40a9-970d-716e4b9d5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [objectives.direction(\"fc\", torch.rand(512, device=device)), objectives.direction(\"avgpool\", torch.rand(512, device=device))]\n",
    "# for s in topk_idx:\n",
    "#     channels.append(s[0] + \":\" + str(s[1][1].item()))\n",
    "\n",
    "fv.visualize_channels(channels, standard_model, output_dir, save=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.11.3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.10.10"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
